# TODO: ollama
# 3. Set up Ollama on your local machine, loading one of the Ollama
# library models.
# 4. Create another variant of your script where you call your local Ollama
# LLM.
# 5. Run 5 of your prompts from last week through your Ollama chatbot.
# See any differences? Regression? Anything you could fix through
# prompt engineering?
